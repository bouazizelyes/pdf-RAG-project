{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b1bef3b-5fbd-4ba4-8117-e54d4e8151c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Aucun fichier PDF trouv√© dans le r√©pertoire : $HOME/stage/pdf-rag-project/data/raw\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm # Pour une belle barre de progression\n",
    "\n",
    "# --- Fonctions de nettoyage (celles que nous avons d√©finies pr√©c√©demment) ---\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'(\\w+)-\\n(\\w+)', r'\\1\\2', text)\n",
    "    text = re.sub(r'Page\\s\\d+\\s(sur|of)\\s\\d+', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# --- La nouvelle fonction principale ---\n",
    "\n",
    "def process_all_pdfs_in_directory(directory_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Scanne un r√©pertoire, trouve tous les fichiers .pdf, en extrait le texte,\n",
    "    le nettoie et retourne une seule cha√Æne de caract√®res contenant tout le texte.\n",
    "    \n",
    "    Args:\n",
    "        directory_path: Le chemin vers le r√©pertoire contenant les PDF.\n",
    "\n",
    "    Returns:\n",
    "        Une cha√Æne de caract√®res unique avec le contenu de tous les PDF.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Utiliser pathlib pour une manipulation de chemin robuste\n",
    "    path = Path(directory_path)\n",
    "    \n",
    "    # 1. Lister tous les fichiers et ne garder que les .pdf\n",
    "    # path.glob('*.pdf') est un g√©n√©rateur qui trouve tous les fichiers correspondant au motif\n",
    "    pdf_files = list(path.glob('*.pdf'))\n",
    "    \n",
    "    if not pdf_files:\n",
    "        print(f\"‚ö†Ô∏è Aucun fichier PDF trouv√© dans le r√©pertoire : {directory_path}\")\n",
    "        return \"\"\n",
    "        \n",
    "    print(f\"‚úÖ Trouv√© {len(pdf_files)} fichier(s) PDF √† traiter.\")\n",
    "    \n",
    "    all_cleaned_text = []\n",
    "    \n",
    "    # 2. Parcourir chaque fichier PDF trouv√© avec une barre de progression\n",
    "    # tqdm rend le processus beaucoup plus agr√©able √† suivre\n",
    "    for pdf_path in tqdm(pdf_files, desc=\"Traitement des PDF\"):\n",
    "        try:\n",
    "            doc = fitz.open(pdf_path)\n",
    "            # 3. Extraire le texte de chaque page\n",
    "            file_text = \"\"\n",
    "            for page in doc:\n",
    "                file_text += page.get_text(\"text\") + \"\\n\"\n",
    "            doc.close()\n",
    "            \n",
    "            # 4. Nettoyer le texte extrait\n",
    "            cleaned_file_text = clean_text(file_text)\n",
    "            all_cleaned_text.append(cleaned_file_text)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # G√©rer les erreurs si un PDF est corrompu ou illisible\n",
    "            print(f\"‚ùå Erreur lors du traitement du fichier {pdf_path.name}: {e}\")\n",
    "            \n",
    "    # 5. Combiner le texte de tous les fichiers en un seul grand texte\n",
    "    # On ajoute un s√©parateur clair pour marquer la fin d'un document\n",
    "    return \"\\n\\n<|END_OF_DOCUMENT|>\\n\\n\".join(all_cleaned_text)\n",
    "\n",
    "\n",
    "# --- Comment l'utiliser dans votre script principal ---\n",
    "\n",
    "# D√©finir le chemin vers votre r√©pertoire de donn√©es\n",
    "RAW_DATA_PATH = \"$HOME/stage/pdf-rag-project/data/raw\" \n",
    "\n",
    "# Appeler la fonction pour obtenir tout le texte\n",
    "full_corpus_text = process_all_pdfs_in_directory(RAW_DATA_PATH)\n",
    "\n",
    "if full_corpus_text:\n",
    "    print(f\"\\nTraitement termin√©. Longueur totale du texte : {len(full_corpus_text)} caract√®res.\")\n",
    "    \n",
    "    # Maintenant, vous pouvez passer `full_corpus_text` √† votre\n",
    "    # `RecursiveCharacterTextSplitter` pour cr√©er les chunks.\n",
    "    # ... la suite de votre pipeline RAG ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1e22140-609c-4f21-acac-33d9ee1b7806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Racine du projet d√©termin√©e : /home/elyes/stage/pdf-rag-project\n",
      "Racine du projet d√©tect√©e : /home/elyes/stage/pdf-rag-project\n",
      "‚úÖ Trouv√© 1 fichier(s) PDF √† traiter dans '/home/elyes/stage/pdf-rag-project/data/raw'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement des PDF: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 50.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Traitement termin√©. Longueur totale du texte : 1522 caract√®res.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import re\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "current_path = Path.cwd()\n",
    "if current_path.name == \"notebooks\":\n",
    "    PROJECT_ROOT = current_path.parent\n",
    "else:\n",
    "    PROJECT_ROOT = current_path\n",
    "    \n",
    "print(f\"Racine du projet d√©termin√©e : {PROJECT_ROOT}\")\n",
    "\n",
    "\n",
    "RAW_DATA_PATH = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "PROCESSED_DATA_PATH = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "\n",
    "PROCESSED_DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    patterns_to_remove = [\n",
    "        r\"IMPACTDEV\\s*D√©veloppement de solutions\",  # Logo et slogan\n",
    "        r\"\\w+-\\w+-\\d{2}-\\d{2}\",                    # ID de document comme DMGP-PR-03-01\n",
    "        r\"Date\\s*:\\s*\\d{2}/\\d{2}/\\d{4}\",          # Date comme Date : 04/03/2025\n",
    "        r\"Page\\s+\\d+\\s+sur\\s+\\d+\"                 # Num√©ro de page comme Page 1 sur 3\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns_to_remove:\n",
    "        text = re.sub(pattern, \"\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'(\\w+)-\\n(\\w+)', r'\\1\\2', text)\n",
    "    text = re.sub(r'Page\\s\\d+\\s(sur|of)\\s\\d+', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# --- Fonction de traitement (inchang√©e, elle prend un chemin en argument) ---\n",
    "def process_all_pdfs_in_directory(directory_path: Path) -> str:\n",
    "    pdf_files = list(directory_path.glob('*.pdf'))\n",
    "    \n",
    "    if not pdf_files:\n",
    "        print(f\"‚ö†Ô∏è Aucun fichier PDF trouv√© dans le r√©pertoire : {directory_path}\")\n",
    "        return \"\"\n",
    "        \n",
    "    print(f\"‚úÖ Trouv√© {len(pdf_files)} fichier(s) PDF √† traiter dans '{directory_path}'.\")\n",
    "    \n",
    "    all_cleaned_text = []\n",
    "    \n",
    "    for pdf_path in tqdm(pdf_files, desc=\"Traitement des PDF\"):\n",
    "        try:\n",
    "            with fitz.open(pdf_path) as doc:\n",
    "                file_text = \"\"\n",
    "                for page in doc:\n",
    "                    file_text += page.get_text(\"text\") + \"\\n\"\n",
    "            \n",
    "            cleaned_file_text = clean_text(file_text)\n",
    "            all_cleaned_text.append(cleaned_file_text)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur lors du traitement du fichier {pdf_path.name}: {e}\")\n",
    "            \n",
    "    return \"\\n\\n<|END_OF_DOCUMENT|>\\n\\n\".join(all_cleaned_text)\n",
    "\n",
    "\n",
    "# --- Point d'entr√©e principal du script ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Racine du projet d√©tect√©e : {PROJECT_ROOT}\")\n",
    "    \n",
    "    # Appeler la fonction principale avec le chemin que nous avons construit\n",
    "    full_corpus_text = process_all_pdfs_in_directory(RAW_DATA_PATH)\n",
    "\n",
    "    if full_corpus_text:\n",
    "        print(f\"\\nTraitement termin√©. Longueur totale du texte : {len(full_corpus_text)} caract√®res.\")\n",
    "        \n",
    "        # Ici, vous continueriez avec le reste de votre pipeline :\n",
    "        # 1. D√©couper `full_corpus_text` en chunks.\n",
    "        # 2. Cr√©er les embeddings pour chaque chunk.\n",
    "        # 3. Construire l'index FAISS.\n",
    "        # 4. Sauvegarder l'index et les chunks dans PROCESSED_DATA_PATH.\n",
    "        \n",
    "        # Exemple de sauvegarde :\n",
    "        # faiss_index_path = PROCESSED_DATA_PATH / \"my_faiss.index\"\n",
    "        # faiss.write_index(index, str(faiss_index_path))\n",
    "        # print(f\"Index FAISS sauvegard√© dans : {faiss_index_path}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Aucun texte n'a √©t√© trait√©. Fin du script.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "954fe806-14d9-43ac-a860-2e561c4b6905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Racine du projet d√©termin√©e : /home/elyes/stage/pdf-rag-project\n",
      "Analyse du fichier de test : DMGP-PR-03-01 Gestion de version de code.pdf\n",
      "--------------------------------------------------\n",
      "üî¨ VISUALISATION DE L'EFFET DU NETTOYAGE üî¨\n",
      "--------------------------------------------------\n",
      "L√©gende : [- Ligne supprim√©e] [+ Ligne ajout√©e]\n",
      "\n",
      "--- Texte Brut\n",
      "+++ Texte Nettoy√©\n",
      "@@ -1,11 +1,9 @@\n",
      "- \n",
      " Proc√©dure \n",
      " DMGP-PR-03-01 \n",
      " Gestion de version de code \n",
      " Date : 04/03/2025 \n",
      " Page 1 sur3 \n",
      "- \n",
      "- \n",
      "+\n",
      " Version actuelle \n",
      " Propri√©taire du \n",
      " document \n",
      "@@ -16,7 +14,7 @@\n",
      " IMPACTDEV \n",
      " Interne \n",
      " Valid√© \n",
      "- \n",
      "+\n",
      " √âtablissement, v√©rification et approbation  \n",
      " R√¥le \n",
      " Nom & Pr√©nom \n",
      "@@ -25,17 +23,16 @@\n",
      " √âtabli par  \n",
      " Faiez KTATA \n",
      " DT \n",
      "- \n",
      "+\n",
      " V√©rification \n",
      " Hana GHRIBI \n",
      " RMQ \n",
      "- \n",
      "+\n",
      " Approbation   \n",
      " Itebeddine GHORBEL \n",
      " Nebras GHARBI \n",
      " DG \n",
      "- \n",
      "- \n",
      "+\n",
      " Historique de versions  \n",
      " Version \n",
      " Date \n",
      "@@ -58,23 +55,13 @@\n",
      " Nebras GHARBI \n",
      " Validation \n",
      " DG \n",
      "- \n",
      "- \n",
      "- \n",
      "- \n",
      "- \n",
      "- \n",
      "- \n",
      "- \n",
      "- \n",
      "+\n",
      " \n",
      "  \n",
      " Proc√©dure \n",
      " DMGP-PR-03-01 \n",
      " Gestion de version de code \n",
      "-Date : 04/03/2025 \n",
      "-Page 2 sur3 \n",
      "- \n",
      "+\n",
      " I. \n",
      " Objet  \n",
      " Cette proc√©dure a pour objectif de garantir une gestion coh√©rente, ordonn√©e et transparente des \n",
      "@@ -95,10 +82,6 @@\n",
      " Proc√©dure \n",
      " DMGP-PR-03-01 \n",
      " Gestion de version de code \n",
      "-Date : 04/03/2025 \n",
      "-Page 3 sur3 \n",
      "- \n",
      "+\n",
      " IV. \n",
      "-D√©roulement (DMGP-DG-04) \n",
      "- \n",
      "-\n",
      "+D√©roulement (DMGP-DG-04)\n",
      "--------------------------------------------------\n",
      "\n",
      " Aper√ßu du d√©but du texte NETTOY√â :\n",
      "'Proc√©dure \n",
      "DMGP-PR-03-01 \n",
      "Gestion de version de code \n",
      "Date : 04/03/2025 \n",
      "Page 1 sur3 \n",
      "\n",
      "Version actuelle \n",
      "Propri√©taire du \n",
      "document \n",
      "Classification de \n",
      "confidentialit√© \n",
      "Statut \n",
      "01 \n",
      "IMPACTDEV \n",
      "Interne \n",
      "...'\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import re\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import difflib\n",
    "\n",
    "# --- D√©finition des chemins (inchang√©) ---\n",
    "current_path = Path.cwd()\n",
    "if current_path.name == \"notebooks\":\n",
    "    PROJECT_ROOT = current_path.parent\n",
    "else:\n",
    "    PROJECT_ROOT = current_path\n",
    "print(f\"Racine du projet d√©termin√©e : {PROJECT_ROOT}\")\n",
    "\n",
    "RAW_DATA_PATH = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "\n",
    "# --- NOUVELLE STRAT√âGIE DE NETTOYAGE PAGE PAR PAGE ---\n",
    "\n",
    "def smart_clean_pdf_text(doc: fitz.Document) -> str:\n",
    "    \"\"\"\n",
    "    Nettoie le texte d'un document PDF en traitant chaque page individuellement\n",
    "    pour supprimer les en-t√™tes et pieds de page, tout en pr√©servant la page de garde.\n",
    "    \"\"\"\n",
    "    \n",
    "    full_cleaned_text = \"\"\n",
    "        \n",
    "    # 1. D√©finir les motifs d'en-t√™te et de pied de page √† supprimer\n",
    "    # On utilise des ancres (^ pour d√©but de ligne, $ pour fin de ligne) pour √™tre plus pr√©cis\n",
    "    header_patterns = [\n",
    "        re.compile(r\"^Proc√©dure\\s*\\w+-\\w+-\\d{2}-\\d{2}\", re.IGNORECASE),\n",
    "        re.compile(r\"^IMPACTDEV D√©veloppement de solutions\", re.IGNORECASE)\n",
    "    ]\n",
    "    footer_patterns = [\n",
    "        re.compile(r\"Page\\s*\\d+\\s*sur\\s*\\d+$\", re.IGNORECASE),\n",
    "        re.compile(r\"Date\\s*:\\s*\\d{2}/\\d{2}/\\d{4}$\", re.IGNORECASE) # Date semble √™tre dans le footer\n",
    "    ]\n",
    "\n",
    "    # 2. Parcourir chaque page du document\n",
    "    for page_num, page in enumerate(doc):\n",
    "        page_text = page.get_text(\"text\")\n",
    "        \n",
    "        # 3. Laisser la premi√®re page (page_num == 0) presque intacte\n",
    "        # Elle contient l'historique des versions que nous voulons garder.\n",
    "        if page_num == 0:\n",
    "            cleaned_page_text = page_text\n",
    "        else:\n",
    "            # Pour les autres pages, on nettoie les en-t√™tes et pieds de page\n",
    "            lines = page_text.split('\\n')\n",
    "            cleaned_lines = []\n",
    "            for line in lines:\n",
    "                is_header_or_footer = False\n",
    "                # V√©rifier si la ligne correspond √† un motif d'en-t√™te ou de pied de page\n",
    "                for pattern in header_patterns + footer_patterns:\n",
    "                    if pattern.search(line.strip()):\n",
    "                        is_header_or_footer = True\n",
    "                        break\n",
    "                \n",
    "                if not is_header_or_footer:\n",
    "                    cleaned_lines.append(line)\n",
    "            \n",
    "            cleaned_page_text = \"\\n\".join(cleaned_lines)\n",
    "\n",
    "        # 4. Nettoyage final pour les espaces, etc.\n",
    "        cleaned_page_text = re.sub(r'(\\w+)-\\n(\\w+)', r'\\1\\2', cleaned_page_text)\n",
    "        cleaned_page_text = re.sub(r'(\\n\\s*){2,}', '\\n\\n', cleaned_page_text) # Garde les doubles sauts de ligne\n",
    "        \n",
    "        full_cleaned_text += cleaned_page_text + \"\\n\"\n",
    "        \n",
    "    return full_cleaned_text.strip()\n",
    "\n",
    "\n",
    "# --- Fonction de visualisation (inchang√©e) ---\n",
    "def visualize_cleaning_effect(raw_text: str, cleaned_text: str, sample_size: int = 2000):\n",
    "    print(\"-\" * 50 + \"\\nüî¨ VISUALISATION DE L'EFFET DU NETTOYAGE üî¨\\n\" + \"-\" * 50)\n",
    "    raw_sample = raw_text[:sample_size]; cleaned_sample = cleaned_text[:sample_size]\n",
    "    raw_lines = raw_sample.splitlines(); cleaned_lines = cleaned_sample.splitlines()\n",
    "    diff = difflib.unified_diff(raw_lines, cleaned_lines, fromfile='Texte Brut', tofile='Texte Nettoy√©', lineterm='')\n",
    "    print(\"L√©gende : [- Ligne supprim√©e] [+ Ligne ajout√©e]\\n\")\n",
    "    for line in diff:\n",
    "        print(line)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# --- Ex√©cution principale pour le test ---\n",
    "\n",
    "pdf_files = list(RAW_DATA_PATH.glob('*.pdf'))\n",
    "\n",
    "if not pdf_files:\n",
    "    print(f\"‚ö†Ô∏è Aucun fichier PDF trouv√© dans {RAW_DATA_PATH}.\")\n",
    "else:\n",
    "    pdf_to_test = pdf_files[0]\n",
    "    print(f\"Analyse du fichier de test : {pdf_to_test.name}\")\n",
    "    \n",
    "    raw_text = \"\"\n",
    "    try:\n",
    "        with fitz.open(pdf_to_test) as doc:\n",
    "            # On passe l'objet 'doc' entier √† notre nouvelle fonction\n",
    "            cleaned_text = smart_clean_pdf_text(doc)\n",
    "            \n",
    "            # Pour la visualisation, on a besoin du texte brut complet\n",
    "            for page in doc:\n",
    "                raw_text += page.get_text(\"text\") + \"\\n\"\n",
    "        \n",
    "        visualize_cleaning_effect(raw_text, cleaned_text)\n",
    "\n",
    "        print(\"\\n Aper√ßu du d√©but du texte NETTOY√â :\")\n",
    "        print(\"'\" + cleaned_text[:200] + \"...'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lors du traitement du fichier {pdf_to_test.name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c55b3b9-bb16-45d8-a9d0-e3da51881def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pr√™t √† passer aux √©tapes de chunking, embedding et indexation.\n",
      "\n",
      "--- √âtape 2 : D√©coupage du texte en chunks ---\n",
      "‚úÖ Le texte a √©t√© d√©coup√© en 3 chunks.\n",
      "\n",
      "--- √âtape 3 : Cr√©ation des embeddings pour chaque chunk ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elyes/miniconda3/envs/llm-dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eccb82500094e30875e71db698ce631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embeddings cr√©√©s. La forme de la matrice de vecteurs est : (3, 384)\n",
      "\n",
      "--- √âtape 4 : Cr√©ation et remplissage de l'index vectoriel FAISS ---\n",
      "‚úÖ Index FAISS cr√©√© avec succ√®s. Il contient 3 vecteurs.\n",
      "\n",
      "--- √âtape 5 : Sauvegarde de l'index et des chunks ---\n",
      "‚úÖ Index sauvegard√© dans : /home/elyes/stage/pdf-rag-project/data/processed/my_documents.index\n",
      "‚úÖ Chunks sauvegard√©s dans : /home/elyes/stage/pdf-rag-project/data/processed/my_documents_chunks.pkl\n",
      "\n",
      "üéâ Le 'cerveau' de votre RAG est pr√™t et sauvegard√© ! üéâ\n"
     ]
    }
   ],
   "source": [
    "# Assurez-vous que ces biblioth√®ques sont bien import√©es\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pickle # Pour sauvegarder nos chunks de texte\n",
    "\n",
    "# --- Point de d√©part : la variable 'full_corpus_text' issue du nettoyage ---\n",
    "# On s'assure qu'elle n'est pas vide avant de continuer.\n",
    "if 'full_corpus_text' in locals() and full_corpus_text:\n",
    "    print(\"Pr√™t √† passer aux √©tapes de chunking, embedding et indexation.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è La variable 'full_corpus_text' est vide ou n'existe pas. Veuillez ex√©cuter la cellule de nettoyage d'abord.\")\n",
    "    # On arr√™te l'ex√©cution de la cellule si le texte n'est pas pr√™t\n",
    "    # (dans un notebook, vous pouvez simplement ne pas ex√©cuter la suite)\n",
    "\n",
    "\n",
    "# === √âTAPE 2 : D√âCOUPAGE DU TEXTE (CHUNKING) ===\n",
    "print(\"\\n--- √âtape 2 : D√©coupage du texte en chunks ---\")\n",
    "\n",
    "# On utilise un d√©coupeur r√©cursif qui essaie de respecter les paragraphes et les phrases.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=700,      # Taille cible de chaque chunk en caract√®res.\n",
    "    chunk_overlap=250,    # Nombre de caract√®res de chevauchement entre les chunks.\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(full_corpus_text)\n",
    "print(f\"‚úÖ Le texte a √©t√© d√©coup√© en {len(chunks)} chunks.\")\n",
    "\n",
    "\n",
    "# === √âTAPE 3 : VECTORISATION (EMBEDDING) ===\n",
    "print(\"\\n--- √âtape 3 : Cr√©ation des embeddings pour chaque chunk ---\")\n",
    "\n",
    "# On charge le mod√®le d'embedding.\n",
    "# 'all-MiniLM-L6-v2' est un excellent choix pour d√©marrer : rapide et performant.\n",
    "# Le mod√®le sera t√©l√©charg√© lors de la premi√®re utilisation.\n",
    "embedding_model = SentenceTransformer('BAAI/bge-small-en-v1.5', device='cuda')\n",
    "\n",
    "# On convertit les chunks en vecteurs. Cette op√©ration peut prendre du temps.\n",
    "# show_progress_bar=True est tr√®s utile pour suivre l'avancement.\n",
    "chunk_embeddings = embedding_model.encode(chunks, show_progress_bar=True)\n",
    "\n",
    "print(f\"‚úÖ Embeddings cr√©√©s. La forme de la matrice de vecteurs est : {chunk_embeddings.shape}\")\n",
    "\n",
    "\n",
    "# === √âTAPE 4 : INDEXATION DANS FAISS ===\n",
    "print(\"\\n--- √âtape 4 : Cr√©ation et remplissage de l'index vectoriel FAISS ---\")\n",
    "\n",
    "# Obtenir la dimension des vecteurs (ex: 384 pour 'all-MiniLM-L6-v2')\n",
    "d = chunk_embeddings.shape[1]\n",
    "\n",
    "# On cr√©e un index FAISS simple mais tr√®s pr√©cis.\n",
    "# IndexFlatL2 calcule la distance exacte entre les vecteurs.\n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# On ajoute nos embeddings √† l'index.\n",
    "# Ils doivent √™tre convertis en float32 pour FAISS.\n",
    "index.add(np.array(chunk_embeddings).astype('float32'))\n",
    "\n",
    "print(f\"‚úÖ Index FAISS cr√©√© avec succ√®s. Il contient {index.ntotal} vecteurs.\")\n",
    "\n",
    "\n",
    "# === √âTAPE 5 : SAUVEGARDE DES ARTEFACTS (TR√àS IMPORTANT !) ===\n",
    "# L'embedding est lent. On sauvegarde les r√©sultats pour ne pas avoir √† le refaire.\n",
    "print(\"\\n--- √âtape 5 : Sauvegarde de l'index et des chunks ---\")\n",
    "\n",
    "# D√©finir les chemins de sauvegarde dans notre dossier de donn√©es trait√©es\n",
    "FAISS_INDEX_PATH = PROCESSED_DATA_PATH / \"my_documents.index\"\n",
    "CHUNKS_PATH = PROCESSED_DATA_PATH / \"my_documents_chunks.pkl\"\n",
    "\n",
    "# Sauvegarder l'index FAISS\n",
    "faiss.write_index(index, str(FAISS_INDEX_PATH))\n",
    "\n",
    "# Sauvegarder la liste des chunks de texte avec pickle\n",
    "with open(CHUNKS_PATH, \"wb\") as f:\n",
    "    pickle.dump(chunks, f)\n",
    "\n",
    "print(f\"‚úÖ Index sauvegard√© dans : {FAISS_INDEX_PATH}\")\n",
    "print(f\"‚úÖ Chunks sauvegard√©s dans : {CHUNKS_PATH}\")\n",
    "print(\"\\nüéâ Le 'cerveau' de votre RAG est pr√™t et sauvegard√© ! üéâ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd16aa9-b553-4374-b0e7-8e8b6ca40940",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
