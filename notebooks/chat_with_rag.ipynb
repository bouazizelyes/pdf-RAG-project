{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d11027-2a85-45d8-8871-873b39e34511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Configuration de l'application RAG...\n",
      "üß† Chargement des mod√®les (LLM et Embedding)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3876be7c580d42128b12d189348e62fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/elyes/miniconda3/envs/llm-dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Mod√®les charg√©s.\n",
      "üìö Chargement de l'index FAISS et des chunks de texte...\n",
      "‚úÖ Index et chunks charg√©s avec succ√®s.\n",
      "\n",
      "==================================================\n",
      "ü§ñ Assistant RAG pr√™t. Posez vos questions sur les documents.\n",
      "Tapez '/exit' pour quitter.\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Vous:  donne moi l'importance ou l'objectif de la Gestion de version de code\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: D√©sol√©, une erreur est survenue lors de la g√©n√©ration de la r√©ponse.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# --- 0. Configuration et D√©finition des Chemins ---\n",
    "print(\"üöÄ Configuration de l'application RAG...\")\n",
    "\n",
    "# D√©finir le chemin racine du projet (ajuster si n√©cessaire)\n",
    "current_path = Path.cwd()\n",
    "if current_path.name == \"notebooks\":\n",
    "    PROJECT_ROOT = current_path.parent\n",
    "else:\n",
    "    PROJECT_ROOT = current_path\n",
    "    \n",
    "PROCESSED_DATA_PATH = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "FAISS_INDEX_PATH = PROCESSED_DATA_PATH / \"my_documents.index\"\n",
    "CHUNKS_PATH = PROCESSED_DATA_PATH / \"my_documents_chunks.pkl\"\n",
    "\n",
    "# --- 1. Chargement des Mod√®les ---\n",
    "print(\"üß† Chargement des mod√®les (LLM et Embedding)...\")\n",
    "\n",
    "# Charger le LLM (Phi-3)\n",
    "llm_model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    llm_model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(llm_model_id, trust_remote_code=True)\n",
    "\n",
    "# Charger le mod√®le d'Embedding\n",
    "embedding_model = SentenceTransformer('BAAI/bge-small-en-v1.5', device='cpu')\n",
    "print(\"‚úÖ Mod√®les charg√©s.\")\n",
    "\n",
    "\n",
    "# --- 2. Chargement des Artefacts RAG (le \"cerveau\") ---\n",
    "print(\"üìö Chargement de l'index FAISS et des chunks de texte...\")\n",
    "try:\n",
    "    index = faiss.read_index(str(FAISS_INDEX_PATH))\n",
    "    with open(CHUNKS_PATH, \"rb\") as f:\n",
    "        chunks = pickle.load(f)\n",
    "    print(\"‚úÖ Index et chunks charg√©s avec succ√®s.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur lors du chargement des fichiers RAG. Avez-vous ex√©cut√© le script de cr√©ation d'index d'abord ?\")\n",
    "    print(f\"Erreur : {e}\")\n",
    "    # Arr√™ter le script si les fichiers ne peuvent pas √™tre charg√©s\n",
    "    exit()\n",
    "\n",
    "# --- 3. D√©finition de la fonction RAG principale ---\n",
    "\n",
    "def answer_question_with_rag(question: str, k: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Prend une question, trouve les chunks pertinents, construit un prompt et g√©n√®re une r√©ponse.\n",
    "    \"\"\"\n",
    "    \n",
    "    # √âtape 3.1 : Vectoriser la question de l'utilisateur\n",
    "    question_embedding = embedding_model.encode([question])\n",
    "    \n",
    "    # √âtape 3.2 : Chercher dans l'index FAISS\n",
    "    distances, indices = index.search(question_embedding.astype('float32'), k)\n",
    "    \n",
    "    # √âtape 3.3 : R√©cup√©rer les chunks de texte pertinents\n",
    "    retrieved_chunks = [chunks[i] for i in indices[0]]\n",
    "    context_text = \"\\n\\n---\\n\\n\".join(retrieved_chunks)\n",
    "    \n",
    "    # √âtape 3.4 : Construire le prompt pour le LLM\n",
    "    # C'est ici que le \"prompt engineering\" est crucial.\n",
    "    prompt_template = f\"\"\"\n",
    "<|system|>\n",
    "Vous √™tes un assistant expert qui r√©pond aux questions de mani√®re pr√©cise en vous basant sur le contexte fourni ci-dessous.\n",
    ".\n",
    "Ne mentionnez pas l'existence du \"contexte\" dans votre r√©ponse. R√©pondez directement √† la question.\n",
    "\n",
    "Contexte fourni :\n",
    "{context_text}<|end|>\n",
    "<|user|>\n",
    "{question}<|end|>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "    \n",
    "    # √âtape 3.5 : G√©n√©rer la r√©ponse avec le LLM\n",
    "    input_ids = llm_tokenizer(prompt_template, return_tensors=\"pt\").to(llm_model.device)\n",
    "    \n",
    "    outputs = llm_model.generate(\n",
    "        **input_ids,\n",
    "        max_new_tokens=512,  # On peut augmenter un peu pour des r√©ponses plus longues\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "        eos_token_id=llm_tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # √âtape 3.6 : D√©coder la r√©ponse\n",
    "    full_response = llm_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extraire uniquement la partie g√©n√©r√©e par l'assistant\n",
    "    # Cette m√©thode est plus simple et robuste\n",
    "    assistant_part = full_response.split(\"<|assistant|>\")\n",
    "    if len(assistant_part) > 1:\n",
    "        return assistant_part[1].strip()\n",
    "    else:\n",
    "        # Fallback si le formatage est inattendu\n",
    "        return \"D√©sol√©, une erreur est survenue lors de la g√©n√©ration de la r√©ponse.\"\n",
    "\n",
    "\n",
    "# --- 4. Boucle de Chat Interactive ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ü§ñ Assistant RAG pr√™t. Posez vos questions sur les documents.\")\n",
    "    print(\"Tapez '/exit' pour quitter.\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "    while True:\n",
    "        user_question = input(\"Vous: \")\n",
    "        if user_question.lower() == '/exit':\n",
    "            break\n",
    "        \n",
    "        # Obtenir la r√©ponse via la pipeline RAG\n",
    "        answer = answer_question_with_rag(user_question)\n",
    "        \n",
    "        print(f\"Assistant: {answer}\\n\")\n",
    "\n",
    "    print(\"üëã Au revoir !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7172d329-8cb8-4b7a-b6e4-bd243ab9291e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgc\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Delete large objects\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# del model\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# del tokenizer\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# del embeddings\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m input_ids\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m outputs\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m question_embedding\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_ids' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Delete large objects\n",
    "\n",
    "# del model\n",
    "# del tokenizer\n",
    "# del embeddings\n",
    "del input_ids\n",
    "del outputs\n",
    "del question_embedding\n",
    "\n",
    "# Force Python's garbage collection\n",
    "gc.collect()\n",
    "\n",
    "# Empty PyTorch's CUDA cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"VRAM cache cleared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0923a619-54df-4eac-ab1b-e878254d7b69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
