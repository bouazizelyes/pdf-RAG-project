{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa611c27-0d83-4736-857a-c498baa2bcff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Librairies import√©es avec succ√®s.\n",
      "-> Racine du projet d√©finie sur : /home/elyes/stage/pdf-RAG-project\n",
      "-> Les PDF bruts seront lus depuis : /home/elyes/stage/pdf-RAG-project/data/raw\n",
      "-> L'index et les segments trait√©s seront sauvegard√©s dans : /home/elyes/stage/pdf-RAG-project/data/processed\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber  # La librairie cl√© pour extraire texte et tableaux\n",
    "import re\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# --- Librairies pour le coeur du pipeline RAG ---\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# L'import peut varier selon la version de langchain\n",
    "try:\n",
    "    from langchain_core.documents import Document\n",
    "except ImportError:\n",
    "    from langchain.schema import Document\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "print(\"‚úÖ Librairies import√©es avec succ√®s.\")\n",
    "\n",
    "# --- 1. D√©finir la structure des r√©pertoires du projet ---\n",
    "# Trouve la racine du projet de mani√®re robuste\n",
    "chemin_actuel = Path.cwd()\n",
    "if chemin_actuel.name == \"notebooks\":\n",
    "    RACINE_PROJET = chemin_actuel.parent\n",
    "else:\n",
    "    RACINE_PROJET = chemin_actuel\n",
    "\n",
    "CHEMIN_DONNEES_BRUTES = RACINE_PROJET / \"data\" / \"raw\"\n",
    "CHEMIN_DONNEES_TRAITEES = RACINE_PROJET / \"data\" / \"processed\"\n",
    "\n",
    "# S'assurer que le dossier pour les donn√©es trait√©es existe\n",
    "CHEMIN_DONNEES_TRAITEES.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"-> Racine du projet d√©finie sur : {RACINE_PROJET}\")\n",
    "print(f\"-> Les PDF bruts seront lus depuis : {CHEMIN_DONNEES_BRUTES}\")\n",
    "print(f\"-> L'index et les segments trait√©s seront sauvegard√©s dans : {CHEMIN_DONNEES_TRAITEES}\")\n",
    "\n",
    "# --- 2. Configuration ---\n",
    "# D√©finir le mod√®le d'embedding que nous utiliserons\n",
    "# paraphrase-multilingual-MiniLM-L12-v2 est un excellent choix pour le fran√ßais.\n",
    "NOM_MODELE_SENTENCE_EMBEDDING ='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2' \n",
    "\n",
    "# --- NOUVEAU : Configuration pour le d√©bogage visuel ---\n",
    "# Mettre sur True pour activer l'affichage de la comparaison avant/apr√®s nettoyage.\n",
    "# Mettre sur False pour le traitement normal et rapide.\n",
    "AFFICHER_COMPARAISON_AVANT_APRES = False\n",
    "\n",
    "# Limiter l'affichage aux N premi√®res pages de chaque PDF pour ne pas inonder la console.\n",
    "PAGES_A_COMPARER_PAR_PDF = 2\n",
    "chunk_size=400   \n",
    "chunk_overlap=80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdfeede4-f639-482c-836d-406b1d2753eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# === D√âBUT DE LA SECTION AM√âLIOR√âE : Fonctions de traitement de PDF ===\n",
    "# ==============================================================================\n",
    "\n",
    "def nettoyer_texte(texte: str) -> str:\n",
    "    \"\"\"\n",
    "    Nettoie une cha√Æne de caract√®res pour la pr√©paration √† l'embedding.\n",
    "    - Supprime les espaces multiples et les sauts de ligne superflus.\n",
    "    - Corrige les c√©sures de mots en fin de ligne (ex: \"docu-\\nment\" -> \"document\").\n",
    "    - Rejoint les lignes d'un m√™me paragraphe.\n",
    "    - Supprime les lignes enti√®rement vides.\n",
    "    \"\"\"\n",
    "    if not texte:\n",
    "        return \"\"\n",
    "        \n",
    "    # 1. Corriger les c√©sures de mots en fin de ligne\n",
    "    texte = re.sub(r'(\\w+)-\\n(\\w+)', r'\\1\\2', texte)\n",
    "    \n",
    "    # 2. Remplacer les sauts de ligne multiples par un double saut de ligne (marqueur de paragraphe)\n",
    "    texte = re.sub(r'\\n\\s*\\n', '\\n\\n', texte)\n",
    "    \n",
    "    # 3. Supprimer les sauts de ligne simples qui ne sont pas pr√©c√©d√©s par un point ou dans une liste.\n",
    "    # Cela permet de joindre les phrases coup√©es par un retour √† la ligne.\n",
    "    lignes = texte.split('\\n')\n",
    "    lignes_corrigees = []\n",
    "    for i, ligne in enumerate(lignes):\n",
    "        # Si la ligne n'est pas une nouvelle ligne de paragraphe et qu'elle n'est pas une liste √† puce\n",
    "        if i > 0 and ligne.strip() and not lignes[i-1].strip().endswith(('.', ':', '?', '!')) and not ligne.strip().startswith(('*', '-', '‚Ä¢')):\n",
    "            # On la colle √† la ligne pr√©c√©dente avec un espace\n",
    "            if lignes_corrigees: # S'assurer que la liste n'est pas vide\n",
    "                 lignes_corrigees[-1] = lignes_corrigees[-1].strip() + \" \" + ligne.strip()\n",
    "            else:\n",
    "                 lignes_corrigees.append(ligne)\n",
    "        else:\n",
    "            lignes_corrigees.append(ligne)\n",
    "    texte = \"\\n\".join(lignes_corrigees)\n",
    "    \n",
    "    # 4. Supprimer les espaces multiples et les tabulations\n",
    "    texte = re.sub(r'[ \\t]+', ' ', texte)\n",
    "    \n",
    "    # 5. Supprimer les lignes ne contenant que des espaces et les espaces en d√©but/fin de ligne\n",
    "    texte = \"\\n\".join([line.strip() for line in texte.split('\\n') if line.strip()])\n",
    "    \n",
    "    return texte.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c47baf1e-1fee-4ecc-85a7-5764e231d130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertir_tableau_en_markdown(tableau: list[list[str]]) -> str:\n",
    "    \"\"\"\n",
    "    Convertit un tableau (liste de listes) en une cha√Æne de caract√®res au format Markdown.\n",
    "    Version am√©lior√©e pour g√©rer les lignes vides et les cellules None.\n",
    "    \"\"\"\n",
    "    # Filtrer les lignes vides ou invalides\n",
    "    lignes_valides = [ligne for ligne in tableau if ligne and any(cell is not None for cell in ligne)]\n",
    "    if not lignes_valides:\n",
    "        return \"\"\n",
    "    \n",
    "    # Nettoyer les cellules individuelles (remplacer None par \"\" et supprimer les sauts de ligne)\n",
    "    lignes_nettoyees = [\n",
    "        [str(cell).replace('\\n', ' ').strip() if cell is not None else \"\" for cell in ligne]\n",
    "        for ligne in lignes_valides\n",
    "    ]\n",
    "\n",
    "    # Construire la table Markdown\n",
    "    entetes = lignes_nettoyees[0]\n",
    "    markdown = \"| \" + \" | \".join(entetes) + \" |\\n\"\n",
    "    markdown += \"| \" + \" | \".join([\"---\"] * len(entetes)) + \" |\\n\"\n",
    "    for ligne in lignes_nettoyees[1:]:\n",
    "        # S'assurer que la ligne a le bon nombre de colonnes\n",
    "        while len(ligne) < len(entetes):\n",
    "            ligne.append(\"\")\n",
    "        markdown += \"| \" + \" | \".join(ligne[:len(entetes)]) + \" |\\n\"\n",
    "        \n",
    "    return markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9220b351-dc23-48a6-a2e9-d22b0fee7050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identifier_entetes_pieds_de_page(pdf: pdfplumber.PDF, pages_a_verifier: int = 3, zone_pourcentage: float = 0.15) -> tuple[set[str], set[str]]:\n",
    "    \"\"\"\n",
    "    Analyse les premi√®res pages d'un PDF pour identifier les en-t√™tes et pieds de page r√©currents.\n",
    "\n",
    "    Args:\n",
    "        pdf: L'objet pdfplumber.PDF ouvert.\n",
    "        pages_a_verifier: Le nombre de pages √† comparer pour trouver des r√©currences (min 2).\n",
    "        zone_pourcentage: Le pourcentage en haut et en bas de la page √† consid√©rer comme zone potentielle.\n",
    "\n",
    "    Returns:\n",
    "        Un tuple contenant deux sets : (lignes_entetes_communes, lignes_pieds_de_page_communs).\n",
    "    \"\"\"\n",
    "    if len(pdf.pages) < 2:\n",
    "        return set(), set()\n",
    "\n",
    "    # S'assurer de ne pas d√©passer le nombre de pages disponibles\n",
    "    pages_a_verifier = min(pages_a_verifier, len(pdf.pages))\n",
    "    \n",
    "    listes_entetes = []\n",
    "    listes_pieds_de_page = []\n",
    "\n",
    "    for i in range(pages_a_verifier):\n",
    "        page = pdf.pages[i]\n",
    "        hauteur_page = page.height\n",
    "        largeur_page = page.width\n",
    "\n",
    "        # D√©finir la \"bo√Æte\" de la zone d'en-t√™te (ex: les 15% sup√©rieurs de la page)\n",
    "        zone_entete_bbox = (0, 0, largeur_page, hauteur_page * zone_pourcentage)\n",
    "        # D√©finir la \"bo√Æte\" de la zone de pied de page (ex: les 15% inf√©rieurs)\n",
    "        zone_pied_de_page_bbox = (0, hauteur_page * (1 - zone_pourcentage), largeur_page, hauteur_page)\n",
    "\n",
    "        # Extraire le texte de ces zones\n",
    "        entete_potentiel = page.crop(zone_entete_bbox).extract_text(x_tolerance=2)\n",
    "        pied_de_page_potentiel = page.crop(zone_pied_de_page_bbox).extract_text(x_tolerance=2)\n",
    "        \n",
    "        # Nettoyer et normaliser le texte pour la comparaison\n",
    "        # On enl√®ve les chiffres pour que \"Page 5\" et \"Page 6\" soient consid√©r√©s comme identiques\n",
    "        if entete_potentiel:\n",
    "            lignes = {re.sub(r'\\d+', '', ligne).strip() for ligne in entete_potentiel.split('\\n') if ligne.strip()}\n",
    "            listes_entetes.append(lignes)\n",
    "\n",
    "        if pied_de_page_potentiel:\n",
    "            lignes = {re.sub(r'\\d+', '', ligne).strip() for ligne in pied_de_page_potentiel.split('\\n') if ligne.strip()}\n",
    "            listes_pieds_de_page.append(lignes)\n",
    "\n",
    "    # Trouver l'intersection : les lignes de texte pr√©sentes dans toutes les zones v√©rifi√©es\n",
    "    entetes_communs = set.intersection(*listes_entetes) if listes_entetes else set()\n",
    "    pieds_de_page_communs = set.intersection(*listes_pieds_de_page) if listes_pieds_de_page else set()\n",
    "    \n",
    "    return entetes_communs, pieds_de_page_communs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09803c02-f80a-4dbd-9e7b-7e0b80e5b85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traiter_pdf_avec_plumber_ameliore(chemin_pdf: Path) -> list[Document]:\n",
    "    \"\"\"\n",
    "    (Version la plus robuste) Traite un PDF avec identification dynamique des en-t√™tes/pieds de page\n",
    "    et inclut une option de d√©bogage pour afficher une comparaison avant/apr√®s.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    with pdfplumber.open(chemin_pdf) as pdf:\n",
    "        entetes_a_supprimer, pieds_de_page_a_supprimer = identifier_entetes_pieds_de_page(pdf)\n",
    "\n",
    "        for num_page, page in enumerate(pdf.pages):\n",
    "            \n",
    "            # --- CODE DE COMPARAISON (partie 1 : capturer \"AVANT\") ---\n",
    "            texte_brut_avant_tout = \"\"\n",
    "            # On ne fait cette op√©ration que si le d√©bogage est activ√© et pour les premi√®res pages\n",
    "            if AFFICHER_COMPARAISON_AVANT_APRES and num_page < PAGES_A_COMPARER_PAR_PDF:\n",
    "                texte_brut_avant_tout = page.extract_text() or \"[Aucun texte brut trouv√© sur cette page]\"\n",
    "            # -------------------------------------------------------------\n",
    "\n",
    "            lignes_de_texte = page.extract_text_lines(layout=True, strip=True)\n",
    "            \n",
    "            lignes_contenu_principal = []\n",
    "            for ligne in lignes_de_texte:\n",
    "                texte_ligne = ligne['text'].strip()\n",
    "                texte_ligne_normalise = re.sub(r'\\d+', '', texte_ligne).strip()\n",
    "                \n",
    "                if texte_ligne_normalise not in entetes_a_supprimer and texte_ligne_normalise not in pieds_de_page_a_supprimer:\n",
    "                    lignes_contenu_principal.append(texte_ligne)\n",
    "            \n",
    "            texte_page_brut = \"\\n\".join(lignes_contenu_principal)\n",
    "            \n",
    "            texte_page_nettoye = nettoyer_texte(texte_page_brut)\n",
    "            \n",
    "            markdown_tableaux = \"\"\n",
    "            tableaux = page.extract_tables()\n",
    "            if tableaux:\n",
    "                for i, tableau in enumerate(tableaux):\n",
    "                    if tableau and len(tableau) > 1:\n",
    "                        md_table = convertir_tableau_en_markdown(tableau)\n",
    "                        if md_table:\n",
    "                            markdown_tableaux += f\"\\n\\n--- Tableau {i+1} ---\\n{md_table}\"\n",
    "            \n",
    "            contenu_page_complet = f\"{texte_page_nettoye}\\n{markdown_tableaux}\".strip()\n",
    "            \n",
    "            # --- CODE DE COMPARAISON (partie 2 : afficher \"AVANT\" et \"APR√àS\") ---\n",
    "            if AFFICHER_COMPARAISON_AVANT_APRES and num_page < PAGES_A_COMPARER_PAR_PDF:\n",
    "                print(\"\\n\" + \"=\"*80)\n",
    "                print(f\"üîé COMPARAISON POUR : '{chemin_pdf.name}', Page {num_page + 1}\")\n",
    "                print(\"=\"*80)\n",
    "                \n",
    "                print(\"\\n--- AVANT NETTOYAGE (Texte brut de la page) ---\\n\")\n",
    "                print(texte_brut_avant_tout)\n",
    "                \n",
    "                print(\"\\n--- APR√àS NETTOYAGE (Contenu final qui sera stock√©) ---\\n\")\n",
    "                print(contenu_page_complet if contenu_page_complet else \"[Page vide apr√®s nettoyage]\")\n",
    "                \n",
    "                print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "            # -------------------------------------------------------------------------\n",
    "            \n",
    "            if contenu_page_complet:\n",
    "                doc = Document(\n",
    "                    page_content=contenu_page_complet,\n",
    "                    metadata={\n",
    "                        \"source\": str(chemin_pdf.name),\n",
    "                        \"page\": num_page + 1\n",
    "                    }\n",
    "                )\n",
    "                documents.append(doc)\n",
    "            \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5666b9c-e623-4055-8ca3-6ee96f07af58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre fonction `creer_documents_depuis_pdfs` est modifi√©e pour appeler la nouvelle fonction\n",
    "def creer_documents_depuis_pdfs(chemin_repertoire: Path) -> list[Document]:\n",
    "    \"\"\"\n",
    "    Scanne un r√©pertoire, traite tous les PDF avec la m√©thode PLUMBER AM√âLIOR√âE,\n",
    "    et retourne une seule liste contenant tous les objets Document.\n",
    "    \"\"\"\n",
    "    tous_les_docs = []\n",
    "    fichiers_pdf = list(chemin_repertoire.glob('*.pdf'))\n",
    "\n",
    "    if not fichiers_pdf:\n",
    "        print(f\"‚ö†Ô∏è Aucun fichier PDF trouv√© dans : {chemin_repertoire}\")\n",
    "        return []\n",
    "\n",
    "    for chemin_pdf in tqdm(fichiers_pdf, desc=\"Traitement de tous les PDF\"):\n",
    "        try:\n",
    "            # ON APPELLE LA NOUVELLE FONCTION AM√âLIOR√âE ICI\n",
    "            docs_pdf = traiter_pdf_avec_plumber_ameliore(chemin_pdf)\n",
    "            tous_les_docs.extend(docs_pdf)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur lors du traitement du fichier {chemin_pdf.name}: {e}\")\n",
    "            \n",
    "    return tous_les_docs\n",
    "\n",
    "# ==============================================================================\n",
    "# === FIN DE LA SECTION AM√âLIOR√âE ===\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "343fe570-a122-40bb-b742-837060210820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- √âTAPE 1 : CR√âATION DES DOCUMENTS (AVEC NETTOYAGE AVANC√â) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement de tous les PDF: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Cr√©ation des documents termin√©e. Nombre total de pages trait√©es : 42.\n",
      "\n",
      "--- Exemple de sortie nettoy√©e (Page 2 du premier PDF si elle existe) ---\n",
      "Proc√©dure DMGP-PR-05-01 I. Objet Cette proc√©dure d√©crit les √©tapes de d√©signation des Tech Leads pour les six prochains mois au sein de l‚Äô√©quipe technique de IMPACTDEV. Son objectif est de garantir une d√©signation transparente, √©quitable et bas√©e sur le vote des d√©veloppeurs.\n",
      "II. Domaine d‚Äôapplication Cette proc√©dure est applicable √† l‚Äô√©quipe technique de IMPACTDEV, incluant les d√©veloppeurs des technologies React, Symfony et WordPress, et est dirig√©e par la direction technique.\n",
      "III. Responsabilit√©s Cette proc√©dure est sous la responsabilit√© du responsable de la direction technique.\n",
      "IV. D√©finitions et abr√©viations DT : Directeur technique Tech lead : (Technical Lead) est un expert technique qui guide l‚Äô√©quipe de d√©veloppement en assurant la qualit√© technique des projets\n",
      "\n",
      "\n",
      "--- Tableau 1 ---\n",
      "|  | Proc√©dure | DMGP-PR-05-01 |\n",
      "| --- | --- | --- |\n",
      "|  | √âlection du Tech Lead | Date : 04/03/2025 Page 2 sur 3 |\n",
      "\n",
      "--- M√©tadonn√©es associ√©es ---\n",
      "{'source': 'DMGP-PR-05-01 Election du Tech Lead.pdf', 'page': 2}\n",
      "\n",
      "--- √âTAPE 2 : D√©coupage des documents en segments ---\n",
      "‚úÖ Documents d√©coup√©s en 255 segments.\n",
      "\n",
      "--- Exemple de Segment ---\n",
      "Proc√©dure DMGP-PR-05-01 Propri√©taire du Classification de Version actuelle Statut document confidentialit√© 01 IMPACTDEV Interne Valid√© √âtablissement, v√©rification et approbation R√¥le Nom & Pr√©nom Fonction Signature √âtabli par Faiez KTATA DT V√©rification Hana GHRIBI RMQ Itebeddine GHORBEL Approbation DG Nebras GHARBI Historique de versions Version Date Auteur Modification Fonction 00 31/12/2024\n",
      "\n",
      "--- M√©tadonn√©es du Segment ---\n",
      "{'source': 'DMGP-PR-05-01 Election du Tech Lead.pdf', 'page': 1}\n",
      "\n",
      "--- √âTAPE 3 : G√©n√©ration des vecteurs avec 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2' ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elyes/miniconda3/envs/llm-dev-old/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2909e7ce8cdf4b86a2d70588879eb2e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vecteurs g√©n√©r√©s. Forme de la matrice de vecteurs : (255, 384)\n",
      "\n",
      "--- √âTAPES 4 & 5 : Cr√©ation de l'index FAISS et sauvegarde des artefacts ---\n",
      "‚úÖ Index sauvegard√© dans : /home/elyes/stage/pdf-RAG-project/data/processed/documents.index\n",
      "‚úÖ Segments (avec m√©tadonn√©es) sauvegard√©s dans : /home/elyes/stage/pdf-RAG-project/data/processed/documents_segments.pkl\n",
      "\n",
      "üéâ Pipeline d'ingestion termin√© ! Le 'cerveau' de votre RAG est pr√™t. üéâ\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- √âTAPE 1 : CR√âATION DES DOCUMENTS (AVEC NETTOYAGE AVANC√â) ---\")\n",
    "# La logique principale ne change pas, elle utilise maintenant les fonctions am√©lior√©es\n",
    "documents = creer_documents_depuis_pdfs(CHEMIN_DONNEES_BRUTES)\n",
    "\n",
    "if documents:\n",
    "    print(f\"\\n‚úÖ Cr√©ation des documents termin√©e. Nombre total de pages trait√©es : {len(documents)}.\")\n",
    "    print(\"\\n--- Exemple de sortie nettoy√©e (Page 2 du premier PDF si elle existe) ---\")\n",
    "    # Afficher la page 2 pour voir l'effet de la suppression d'en-t√™te\n",
    "    doc_a_afficher = documents[1] if len(documents) > 1 else documents[0]\n",
    "    print(doc_a_afficher.page_content)\n",
    "    print(\"\\n--- M√©tadonn√©es associ√©es ---\")\n",
    "    print(doc_a_afficher.metadata)\n",
    "else:\n",
    "    print(\"\\n‚ùå La cr√©ation des documents a √©chou√© ou aucun texte n'a √©t√© extrait.\")\n",
    "\n",
    "if documents:\n",
    "    # === √âTAPE 2 : D√âCOUPAGE EN SEGMENTS (CHUNKING) ===\n",
    "    print(\"\\n--- √âTAPE 2 : D√©coupage des documents en segments ---\")\n",
    "    diviseur_texte = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,       # Taille de chaque segment en caract√®res\n",
    "        chunk_overlap=chunk_overlap,    # Chevauchement entre les segments\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"] # S√©parateurs tr√®s efficaces\n",
    "    )\n",
    "    segments_finaux = diviseur_texte.split_documents(documents)\n",
    "    print(f\"‚úÖ Documents d√©coup√©s en {len(segments_finaux)} segments.\")\n",
    "    print(\"\\n--- Exemple de Segment ---\")\n",
    "    print(segments_finaux[0].page_content)\n",
    "    print(\"\\n--- M√©tadonn√©es du Segment ---\")\n",
    "    print(segments_finaux[0].metadata)\n",
    "\n",
    "    # === √âTAPE 3 : VECTORISATION (EMBEDDING) ===\n",
    "    print(f\"\\n--- √âTAPE 3 : G√©n√©ration des vecteurs avec '{NOM_MODELE_SENTENCE_EMBEDDING}' ---\")\n",
    "    modele_embedding = SentenceTransformer(NOM_MODELE_SENTENCE_EMBEDDING, device='cuda')\n",
    "    \n",
    "    contenus_segments = [segment.page_content for segment in segments_finaux]\n",
    "    embeddings_segments = modele_embedding.encode(contenus_segments, show_progress_bar=True, normalize_embeddings=True)\n",
    "    print(f\"‚úÖ Vecteurs g√©n√©r√©s. Forme de la matrice de vecteurs : {embeddings_segments.shape}\")\n",
    "\n",
    "    # === √âTAPES 4 & 5 : INDEXATION et SAUVEGARDE ===\n",
    "    print(\"\\n--- √âTAPES 4 & 5 : Cr√©ation de l'index FAISS et sauvegarde des artefacts ---\")\n",
    "    dimension_vecteurs = embeddings_segments.shape[1]\n",
    "    index = faiss.IndexIDMap(faiss.IndexFlatIP(dimension_vecteurs))\n",
    "    index.add_with_ids(embeddings_segments.astype('float32'), np.arange(len(segments_finaux)))\n",
    "\n",
    "    CHEMIN_INDEX_FAISS = CHEMIN_DONNEES_TRAITEES / \"documents.index\"\n",
    "    CHEMIN_SEGMENTS = CHEMIN_DONNEES_TRAITEES / \"documents_segments.pkl\"\n",
    "\n",
    "    faiss.write_index(index, str(CHEMIN_INDEX_FAISS))\n",
    "    with open(CHEMIN_SEGMENTS, \"wb\") as f:\n",
    "        pickle.dump(segments_finaux, f)\n",
    "\n",
    "    print(f\"‚úÖ Index sauvegard√© dans : {CHEMIN_INDEX_FAISS}\")\n",
    "    print(f\"‚úÖ Segments (avec m√©tadonn√©es) sauvegard√©s dans : {CHEMIN_SEGMENTS}\")\n",
    "    print(\"\\nüéâ Pipeline d'ingestion termin√© ! Le 'cerveau' de votre RAG est pr√™t. üéâ\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Le d√©coupage et la vectorisation sont ignor√©s car aucun document n'a √©t√© cr√©√©.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac47897-bcb3-451d-a924-c2d647b6082d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16254d1f-2e5c-4e9f-a4cb-165638e66e31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c60a75b-87fc-4be9-aa7b-81d282326db8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037136cc-3fcf-4e3b-87a8-f7e5cae43cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
