{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa611c27-0d83-4736-857a-c498baa2bcff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Librairies importées avec succès.\n",
      "-> Racine du projet définie sur : /home/elyes/stage/pdf-RAG-project\n",
      "-> Les PDF bruts seront lus depuis : /home/elyes/stage/pdf-RAG-project/data/raw\n",
      "-> L'index et les segments traités seront sauvegardés dans : /home/elyes/stage/pdf-RAG-project/data/processed\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber  # La librairie clé pour extraire texte et tableaux\n",
    "import re\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# --- Librairies pour le coeur du pipeline RAG ---\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# L'import peut varier selon la version de langchain\n",
    "try:\n",
    "    from langchain_core.documents import Document\n",
    "except ImportError:\n",
    "    from langchain.schema import Document\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "print(\"✅ Librairies importées avec succès.\")\n",
    "\n",
    "# --- 1. Définir la structure des répertoires du projet ---\n",
    "# Trouve la racine du projet de manière robuste\n",
    "chemin_actuel = Path.cwd()\n",
    "if chemin_actuel.name == \"notebooks\":\n",
    "    RACINE_PROJET = chemin_actuel.parent\n",
    "else:\n",
    "    RACINE_PROJET = chemin_actuel\n",
    "\n",
    "CHEMIN_DONNEES_BRUTES = RACINE_PROJET / \"data\" / \"raw\"\n",
    "CHEMIN_DONNEES_TRAITEES = RACINE_PROJET / \"data\" / \"processed\"\n",
    "\n",
    "# S'assurer que le dossier pour les données traitées existe\n",
    "CHEMIN_DONNEES_TRAITEES.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"-> Racine du projet définie sur : {RACINE_PROJET}\")\n",
    "print(f\"-> Les PDF bruts seront lus depuis : {CHEMIN_DONNEES_BRUTES}\")\n",
    "print(f\"-> L'index et les segments traités seront sauvegardés dans : {CHEMIN_DONNEES_TRAITEES}\")\n",
    "\n",
    "# --- 2. Configuration ---\n",
    "# Définir le modèle d'embedding que nous utiliserons\n",
    "# paraphrase-multilingual-MiniLM-L12-v2 est un excellent choix pour le français.\n",
    "NOM_MODELE_SENTENCE_EMBEDDING ='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2' \n",
    "\n",
    "# --- NOUVEAU : Configuration pour le débogage visuel ---\n",
    "# Mettre sur True pour activer l'affichage de la comparaison avant/après nettoyage.\n",
    "# Mettre sur False pour le traitement normal et rapide.\n",
    "AFFICHER_COMPARAISON_AVANT_APRES = False\n",
    "\n",
    "# Limiter l'affichage aux N premières pages de chaque PDF pour ne pas inonder la console.\n",
    "PAGES_A_COMPARER_PAR_PDF = 2\n",
    "chunk_size=400   \n",
    "chunk_overlap=80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdfeede4-f639-482c-836d-406b1d2753eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# === DÉBUT DE LA SECTION AMÉLIORÉE : Fonctions de traitement de PDF ===\n",
    "# ==============================================================================\n",
    "\n",
    "def nettoyer_texte(texte: str) -> str:\n",
    "    \"\"\"\n",
    "    Nettoie une chaîne de caractères pour la préparation à l'embedding.\n",
    "    - Supprime les espaces multiples et les sauts de ligne superflus.\n",
    "    - Corrige les césures de mots en fin de ligne (ex: \"docu-\\nment\" -> \"document\").\n",
    "    - Rejoint les lignes d'un même paragraphe.\n",
    "    - Supprime les lignes entièrement vides.\n",
    "    \"\"\"\n",
    "    if not texte:\n",
    "        return \"\"\n",
    "        \n",
    "    # 1. Corriger les césures de mots en fin de ligne\n",
    "    texte = re.sub(r'(\\w+)-\\n(\\w+)', r'\\1\\2', texte)\n",
    "    \n",
    "    # 2. Remplacer les sauts de ligne multiples par un double saut de ligne (marqueur de paragraphe)\n",
    "    texte = re.sub(r'\\n\\s*\\n', '\\n\\n', texte)\n",
    "    \n",
    "    # 3. Supprimer les sauts de ligne simples qui ne sont pas précédés par un point ou dans une liste.\n",
    "    # Cela permet de joindre les phrases coupées par un retour à la ligne.\n",
    "    lignes = texte.split('\\n')\n",
    "    lignes_corrigees = []\n",
    "    for i, ligne in enumerate(lignes):\n",
    "        # Si la ligne n'est pas une nouvelle ligne de paragraphe et qu'elle n'est pas une liste à puce\n",
    "        if i > 0 and ligne.strip() and not lignes[i-1].strip().endswith(('.', ':', '?', '!')) and not ligne.strip().startswith(('*', '-', '•')):\n",
    "            # On la colle à la ligne précédente avec un espace\n",
    "            if lignes_corrigees: # S'assurer que la liste n'est pas vide\n",
    "                 lignes_corrigees[-1] = lignes_corrigees[-1].strip() + \" \" + ligne.strip()\n",
    "            else:\n",
    "                 lignes_corrigees.append(ligne)\n",
    "        else:\n",
    "            lignes_corrigees.append(ligne)\n",
    "    texte = \"\\n\".join(lignes_corrigees)\n",
    "    \n",
    "    # 4. Supprimer les espaces multiples et les tabulations\n",
    "    texte = re.sub(r'[ \\t]+', ' ', texte)\n",
    "    \n",
    "    # 5. Supprimer les lignes ne contenant que des espaces et les espaces en début/fin de ligne\n",
    "    texte = \"\\n\".join([line.strip() for line in texte.split('\\n') if line.strip()])\n",
    "    \n",
    "    return texte.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c47baf1e-1fee-4ecc-85a7-5764e231d130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertir_tableau_en_markdown(tableau: list[list[str]]) -> str:\n",
    "    \"\"\"\n",
    "    Convertit un tableau (liste de listes) en une chaîne de caractères au format Markdown.\n",
    "    Version améliorée pour gérer les lignes vides et les cellules None.\n",
    "    \"\"\"\n",
    "    # Filtrer les lignes vides ou invalides\n",
    "    lignes_valides = [ligne for ligne in tableau if ligne and any(cell is not None for cell in ligne)]\n",
    "    if not lignes_valides:\n",
    "        return \"\"\n",
    "    \n",
    "    # Nettoyer les cellules individuelles (remplacer None par \"\" et supprimer les sauts de ligne)\n",
    "    lignes_nettoyees = [\n",
    "        [str(cell).replace('\\n', ' ').strip() if cell is not None else \"\" for cell in ligne]\n",
    "        for ligne in lignes_valides\n",
    "    ]\n",
    "\n",
    "    # Construire la table Markdown\n",
    "    entetes = lignes_nettoyees[0]\n",
    "    markdown = \"| \" + \" | \".join(entetes) + \" |\\n\"\n",
    "    markdown += \"| \" + \" | \".join([\"---\"] * len(entetes)) + \" |\\n\"\n",
    "    for ligne in lignes_nettoyees[1:]:\n",
    "        # S'assurer que la ligne a le bon nombre de colonnes\n",
    "        while len(ligne) < len(entetes):\n",
    "            ligne.append(\"\")\n",
    "        markdown += \"| \" + \" | \".join(ligne[:len(entetes)]) + \" |\\n\"\n",
    "        \n",
    "    return markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9220b351-dc23-48a6-a2e9-d22b0fee7050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identifier_entetes_pieds_de_page(pdf: pdfplumber.PDF, pages_a_verifier: int = 3, zone_pourcentage: float = 0.15) -> tuple[set[str], set[str]]:\n",
    "    \"\"\"\n",
    "    Analyse les premières pages d'un PDF pour identifier les en-têtes et pieds de page récurrents.\n",
    "\n",
    "    Args:\n",
    "        pdf: L'objet pdfplumber.PDF ouvert.\n",
    "        pages_a_verifier: Le nombre de pages à comparer pour trouver des récurrences (min 2).\n",
    "        zone_pourcentage: Le pourcentage en haut et en bas de la page à considérer comme zone potentielle.\n",
    "\n",
    "    Returns:\n",
    "        Un tuple contenant deux sets : (lignes_entetes_communes, lignes_pieds_de_page_communs).\n",
    "    \"\"\"\n",
    "    if len(pdf.pages) < 2:\n",
    "        return set(), set()\n",
    "\n",
    "    # S'assurer de ne pas dépasser le nombre de pages disponibles\n",
    "    pages_a_verifier = min(pages_a_verifier, len(pdf.pages))\n",
    "    \n",
    "    listes_entetes = []\n",
    "    listes_pieds_de_page = []\n",
    "\n",
    "    for i in range(pages_a_verifier):\n",
    "        page = pdf.pages[i]\n",
    "        hauteur_page = page.height\n",
    "        largeur_page = page.width\n",
    "\n",
    "        # Définir la \"boîte\" de la zone d'en-tête (ex: les 15% supérieurs de la page)\n",
    "        zone_entete_bbox = (0, 0, largeur_page, hauteur_page * zone_pourcentage)\n",
    "        # Définir la \"boîte\" de la zone de pied de page (ex: les 15% inférieurs)\n",
    "        zone_pied_de_page_bbox = (0, hauteur_page * (1 - zone_pourcentage), largeur_page, hauteur_page)\n",
    "\n",
    "        # Extraire le texte de ces zones\n",
    "        entete_potentiel = page.crop(zone_entete_bbox).extract_text(x_tolerance=2)\n",
    "        pied_de_page_potentiel = page.crop(zone_pied_de_page_bbox).extract_text(x_tolerance=2)\n",
    "        \n",
    "        # Nettoyer et normaliser le texte pour la comparaison\n",
    "        # On enlève les chiffres pour que \"Page 5\" et \"Page 6\" soient considérés comme identiques\n",
    "        if entete_potentiel:\n",
    "            lignes = {re.sub(r'\\d+', '', ligne).strip() for ligne in entete_potentiel.split('\\n') if ligne.strip()}\n",
    "            listes_entetes.append(lignes)\n",
    "\n",
    "        if pied_de_page_potentiel:\n",
    "            lignes = {re.sub(r'\\d+', '', ligne).strip() for ligne in pied_de_page_potentiel.split('\\n') if ligne.strip()}\n",
    "            listes_pieds_de_page.append(lignes)\n",
    "\n",
    "    # Trouver l'intersection : les lignes de texte présentes dans toutes les zones vérifiées\n",
    "    entetes_communs = set.intersection(*listes_entetes) if listes_entetes else set()\n",
    "    pieds_de_page_communs = set.intersection(*listes_pieds_de_page) if listes_pieds_de_page else set()\n",
    "    \n",
    "    return entetes_communs, pieds_de_page_communs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09803c02-f80a-4dbd-9e7b-7e0b80e5b85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traiter_pdf_avec_plumber_ameliore(chemin_pdf: Path) -> list[Document]:\n",
    "    \"\"\"\n",
    "    (Version la plus robuste) Traite un PDF avec identification dynamique des en-têtes/pieds de page\n",
    "    et inclut une option de débogage pour afficher une comparaison avant/après.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    with pdfplumber.open(chemin_pdf) as pdf:\n",
    "        entetes_a_supprimer, pieds_de_page_a_supprimer = identifier_entetes_pieds_de_page(pdf)\n",
    "\n",
    "        for num_page, page in enumerate(pdf.pages):\n",
    "            \n",
    "            # --- CODE DE COMPARAISON (partie 1 : capturer \"AVANT\") ---\n",
    "            texte_brut_avant_tout = \"\"\n",
    "            # On ne fait cette opération que si le débogage est activé et pour les premières pages\n",
    "            if AFFICHER_COMPARAISON_AVANT_APRES and num_page < PAGES_A_COMPARER_PAR_PDF:\n",
    "                texte_brut_avant_tout = page.extract_text() or \"[Aucun texte brut trouvé sur cette page]\"\n",
    "            # -------------------------------------------------------------\n",
    "\n",
    "            lignes_de_texte = page.extract_text_lines(layout=True, strip=True)\n",
    "            \n",
    "            lignes_contenu_principal = []\n",
    "            for ligne in lignes_de_texte:\n",
    "                texte_ligne = ligne['text'].strip()\n",
    "                texte_ligne_normalise = re.sub(r'\\d+', '', texte_ligne).strip()\n",
    "                \n",
    "                if texte_ligne_normalise not in entetes_a_supprimer and texte_ligne_normalise not in pieds_de_page_a_supprimer:\n",
    "                    lignes_contenu_principal.append(texte_ligne)\n",
    "            \n",
    "            texte_page_brut = \"\\n\".join(lignes_contenu_principal)\n",
    "            \n",
    "            texte_page_nettoye = nettoyer_texte(texte_page_brut)\n",
    "            \n",
    "            markdown_tableaux = \"\"\n",
    "            tableaux = page.extract_tables()\n",
    "            if tableaux:\n",
    "                for i, tableau in enumerate(tableaux):\n",
    "                    if tableau and len(tableau) > 1:\n",
    "                        md_table = convertir_tableau_en_markdown(tableau)\n",
    "                        if md_table:\n",
    "                            markdown_tableaux += f\"\\n\\n--- Tableau {i+1} ---\\n{md_table}\"\n",
    "            \n",
    "            contenu_page_complet = f\"{texte_page_nettoye}\\n{markdown_tableaux}\".strip()\n",
    "            \n",
    "            # --- CODE DE COMPARAISON (partie 2 : afficher \"AVANT\" et \"APRÈS\") ---\n",
    "            if AFFICHER_COMPARAISON_AVANT_APRES and num_page < PAGES_A_COMPARER_PAR_PDF:\n",
    "                print(\"\\n\" + \"=\"*80)\n",
    "                print(f\"🔎 COMPARAISON POUR : '{chemin_pdf.name}', Page {num_page + 1}\")\n",
    "                print(\"=\"*80)\n",
    "                \n",
    "                print(\"\\n--- AVANT NETTOYAGE (Texte brut de la page) ---\\n\")\n",
    "                print(texte_brut_avant_tout)\n",
    "                \n",
    "                print(\"\\n--- APRÈS NETTOYAGE (Contenu final qui sera stocké) ---\\n\")\n",
    "                print(contenu_page_complet if contenu_page_complet else \"[Page vide après nettoyage]\")\n",
    "                \n",
    "                print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "            # -------------------------------------------------------------------------\n",
    "            \n",
    "            if contenu_page_complet:\n",
    "                doc = Document(\n",
    "                    page_content=contenu_page_complet,\n",
    "                    metadata={\n",
    "                        \"source\": str(chemin_pdf.name),\n",
    "                        \"page\": num_page + 1\n",
    "                    }\n",
    "                )\n",
    "                documents.append(doc)\n",
    "            \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5666b9c-e623-4055-8ca3-6ee96f07af58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre fonction `creer_documents_depuis_pdfs` est modifiée pour appeler la nouvelle fonction\n",
    "def creer_documents_depuis_pdfs(chemin_repertoire: Path) -> list[Document]:\n",
    "    \"\"\"\n",
    "    Scanne un répertoire, traite tous les PDF avec la méthode PLUMBER AMÉLIORÉE,\n",
    "    et retourne une seule liste contenant tous les objets Document.\n",
    "    \"\"\"\n",
    "    tous_les_docs = []\n",
    "    fichiers_pdf = list(chemin_repertoire.glob('*.pdf'))\n",
    "\n",
    "    if not fichiers_pdf:\n",
    "        print(f\"⚠️ Aucun fichier PDF trouvé dans : {chemin_repertoire}\")\n",
    "        return []\n",
    "\n",
    "    for chemin_pdf in tqdm(fichiers_pdf, desc=\"Traitement de tous les PDF\"):\n",
    "        try:\n",
    "            # ON APPELLE LA NOUVELLE FONCTION AMÉLIORÉE ICI\n",
    "            docs_pdf = traiter_pdf_avec_plumber_ameliore(chemin_pdf)\n",
    "            tous_les_docs.extend(docs_pdf)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erreur lors du traitement du fichier {chemin_pdf.name}: {e}\")\n",
    "            \n",
    "    return tous_les_docs\n",
    "\n",
    "# ==============================================================================\n",
    "# === FIN DE LA SECTION AMÉLIORÉE ===\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "343fe570-a122-40bb-b742-837060210820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ÉTAPE 1 : CRÉATION DES DOCUMENTS (AVEC NETTOYAGE AVANCÉ) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement de tous les PDF: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:02<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Création des documents terminée. Nombre total de pages traitées : 42.\n",
      "\n",
      "--- Exemple de sortie nettoyée (Page 2 du premier PDF si elle existe) ---\n",
      "Procédure DMGP-PR-05-01 I. Objet Cette procédure décrit les étapes de désignation des Tech Leads pour les six prochains mois au sein de l’équipe technique de IMPACTDEV. Son objectif est de garantir une désignation transparente, équitable et basée sur le vote des développeurs.\n",
      "II. Domaine d’application Cette procédure est applicable à l’équipe technique de IMPACTDEV, incluant les développeurs des technologies React, Symfony et WordPress, et est dirigée par la direction technique.\n",
      "III. Responsabilités Cette procédure est sous la responsabilité du responsable de la direction technique.\n",
      "IV. Définitions et abréviations DT : Directeur technique Tech lead : (Technical Lead) est un expert technique qui guide l’équipe de développement en assurant la qualité technique des projets\n",
      "\n",
      "\n",
      "--- Tableau 1 ---\n",
      "|  | Procédure | DMGP-PR-05-01 |\n",
      "| --- | --- | --- |\n",
      "|  | Élection du Tech Lead | Date : 04/03/2025 Page 2 sur 3 |\n",
      "\n",
      "--- Métadonnées associées ---\n",
      "{'source': 'DMGP-PR-05-01 Election du Tech Lead.pdf', 'page': 2}\n",
      "\n",
      "--- ÉTAPE 2 : Découpage des documents en segments ---\n",
      "✅ Documents découpés en 255 segments.\n",
      "\n",
      "--- Exemple de Segment ---\n",
      "Procédure DMGP-PR-05-01 Propriétaire du Classification de Version actuelle Statut document confidentialité 01 IMPACTDEV Interne Validé Établissement, vérification et approbation Rôle Nom & Prénom Fonction Signature Établi par Faiez KTATA DT Vérification Hana GHRIBI RMQ Itebeddine GHORBEL Approbation DG Nebras GHARBI Historique de versions Version Date Auteur Modification Fonction 00 31/12/2024\n",
      "\n",
      "--- Métadonnées du Segment ---\n",
      "{'source': 'DMGP-PR-05-01 Election du Tech Lead.pdf', 'page': 1}\n",
      "\n",
      "--- ÉTAPE 3 : Génération des vecteurs avec 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2' ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elyes/miniconda3/envs/llm-dev-old/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2909e7ce8cdf4b86a2d70588879eb2e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vecteurs générés. Forme de la matrice de vecteurs : (255, 384)\n",
      "\n",
      "--- ÉTAPES 4 & 5 : Création de l'index FAISS et sauvegarde des artefacts ---\n",
      "✅ Index sauvegardé dans : /home/elyes/stage/pdf-RAG-project/data/processed/documents.index\n",
      "✅ Segments (avec métadonnées) sauvegardés dans : /home/elyes/stage/pdf-RAG-project/data/processed/documents_segments.pkl\n",
      "\n",
      "🎉 Pipeline d'ingestion terminé ! Le 'cerveau' de votre RAG est prêt. 🎉\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- ÉTAPE 1 : CRÉATION DES DOCUMENTS (AVEC NETTOYAGE AVANCÉ) ---\")\n",
    "# La logique principale ne change pas, elle utilise maintenant les fonctions améliorées\n",
    "documents = creer_documents_depuis_pdfs(CHEMIN_DONNEES_BRUTES)\n",
    "\n",
    "if documents:\n",
    "    print(f\"\\n✅ Création des documents terminée. Nombre total de pages traitées : {len(documents)}.\")\n",
    "    print(\"\\n--- Exemple de sortie nettoyée (Page 2 du premier PDF si elle existe) ---\")\n",
    "    # Afficher la page 2 pour voir l'effet de la suppression d'en-tête\n",
    "    doc_a_afficher = documents[1] if len(documents) > 1 else documents[0]\n",
    "    print(doc_a_afficher.page_content)\n",
    "    print(\"\\n--- Métadonnées associées ---\")\n",
    "    print(doc_a_afficher.metadata)\n",
    "else:\n",
    "    print(\"\\n❌ La création des documents a échoué ou aucun texte n'a été extrait.\")\n",
    "\n",
    "if documents:\n",
    "    # === ÉTAPE 2 : DÉCOUPAGE EN SEGMENTS (CHUNKING) ===\n",
    "    print(\"\\n--- ÉTAPE 2 : Découpage des documents en segments ---\")\n",
    "    diviseur_texte = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,       # Taille de chaque segment en caractères\n",
    "        chunk_overlap=chunk_overlap,    # Chevauchement entre les segments\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"] # Séparateurs très efficaces\n",
    "    )\n",
    "    segments_finaux = diviseur_texte.split_documents(documents)\n",
    "    print(f\"✅ Documents découpés en {len(segments_finaux)} segments.\")\n",
    "    print(\"\\n--- Exemple de Segment ---\")\n",
    "    print(segments_finaux[0].page_content)\n",
    "    print(\"\\n--- Métadonnées du Segment ---\")\n",
    "    print(segments_finaux[0].metadata)\n",
    "\n",
    "    # === ÉTAPE 3 : VECTORISATION (EMBEDDING) ===\n",
    "    print(f\"\\n--- ÉTAPE 3 : Génération des vecteurs avec '{NOM_MODELE_SENTENCE_EMBEDDING}' ---\")\n",
    "    modele_embedding = SentenceTransformer(NOM_MODELE_SENTENCE_EMBEDDING, device='cuda')\n",
    "    \n",
    "    contenus_segments = [segment.page_content for segment in segments_finaux]\n",
    "    embeddings_segments = modele_embedding.encode(contenus_segments, show_progress_bar=True, normalize_embeddings=True)\n",
    "    print(f\"✅ Vecteurs générés. Forme de la matrice de vecteurs : {embeddings_segments.shape}\")\n",
    "\n",
    "    # === ÉTAPES 4 & 5 : INDEXATION et SAUVEGARDE ===\n",
    "    print(\"\\n--- ÉTAPES 4 & 5 : Création de l'index FAISS et sauvegarde des artefacts ---\")\n",
    "    dimension_vecteurs = embeddings_segments.shape[1]\n",
    "    index = faiss.IndexIDMap(faiss.IndexFlatIP(dimension_vecteurs))\n",
    "    index.add_with_ids(embeddings_segments.astype('float32'), np.arange(len(segments_finaux)))\n",
    "\n",
    "    CHEMIN_INDEX_FAISS = CHEMIN_DONNEES_TRAITEES / \"documents.index\"\n",
    "    CHEMIN_SEGMENTS = CHEMIN_DONNEES_TRAITEES / \"documents_segments.pkl\"\n",
    "\n",
    "    faiss.write_index(index, str(CHEMIN_INDEX_FAISS))\n",
    "    with open(CHEMIN_SEGMENTS, \"wb\") as f:\n",
    "        pickle.dump(segments_finaux, f)\n",
    "\n",
    "    print(f\"✅ Index sauvegardé dans : {CHEMIN_INDEX_FAISS}\")\n",
    "    print(f\"✅ Segments (avec métadonnées) sauvegardés dans : {CHEMIN_SEGMENTS}\")\n",
    "    print(\"\\n🎉 Pipeline d'ingestion terminé ! Le 'cerveau' de votre RAG est prêt. 🎉\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ Le découpage et la vectorisation sont ignorés car aucun document n'a été créé.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac47897-bcb3-451d-a924-c2d647b6082d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16254d1f-2e5c-4e9f-a4cb-165638e66e31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c60a75b-87fc-4be9-aa7b-81d282326db8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037136cc-3fcf-4e3b-87a8-f7e5cae43cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
